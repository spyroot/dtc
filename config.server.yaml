train: True
use_dataset: 'lj_speech_625'      # dataset set generated.
use_model: 'dts'                    # model to use , it must be defined in models section.
draw_prediction: True               # at the of training draw.
load_model: True            # load model or not, and what
load_epoch: 500             # load model,  last epoch
save_model: True            # save model,
regenerate: True            # regenerated,  factor when indicated by epochs_save
active_setting: small       # indicate what setting to use, so we can switch from debug to production
evaluate: True              # will run evaluation

root_dir: "."
log_dir: "logs"
nil_dir: "timing"
graph_dir: "graphs"
results_dir: "results"
figures_dir: "figures"
prediction_dir: "prediction"               # where we save prediction
model_save_dir: "model"                    # where we save model
metrics_dir: "metrics"                     # metrics dir in case we need store separate metrics

datasets:
  LJSpeech:
    ds_type: "audio"
    file_type: "wav"
    format: raw
    dir: "~/src/dtc/dataset/LJSpeech-1.1"
    training_meta: ljs_audio_text_train_filelist.txt
    validation_meta: ljs_audio_text_val_filelist.txt
    test_meta: ljs_audio_text_test_filelist.txt
    meta: metadata.csv
    recursive: False
  lj_speech_full:
    format: tensor_mel
    ds_type: "audio"
    dir: "~/src/dtc/dataset/LJSpeech-1.1"
    meta: metadata.csv
    recursive: False
    file_type: "torch"
    dataset_files:
      - "LJSpeech_train_num_sam_12500_filter_80_3.pt"
      - "LJSpeech_validate_num_sam_100_filter_80_3.pt"
      - "LJSpeech_test_num_sam_500_filter_80_3.pt"
    training_meta: LJSpeech_train_num_sam_12500_filter_80_3.pt
    validation_meta: LJSpeech_validate_num_sam_100_filter_80_3.pt
    test_meta: LJSpeech_test_num_sam_500_filter_80_3.pt
    checksums:
      - "7bc0f1bac289cfd1ba8ea1c390bddf8f"
      - "5d3b94b131c08afcca993dfbec54c63a"
      - "3291d802351928da7c5d0d9c917c2663"
  lj_speech_1k_raw:
    ds_type: "audio"
    file_type: "wav"
    format: raw
    dir: "~/src/dtc/dataset/LJSpeech-1.1"
    #    dir: "~/Dropbox/Datasets/LJSpeechSmall"
    training_meta: ljs_audio_text_train_filelist_1000.txt
    validation_meta: ljs_audio_text_val_filelist.txt
    test_meta: ljs_audio_text_test_filelist.txt
    meta: metadata.csv
    recursive: False
  lj_speech_625:
    format: tensor_mel
    ds_type: "audio"
    dir: "~/src/dtc/dataset/LJSpeech-1.1"
    dataset_files:
      - "LJSpeech_train_num_sam_625_filter_80_3.pt"
      - "LJSpeech_validate_num_sam_100_filter_80_3.pt"
      - "LJSpeech_test_num_sam_25_filter_80_3.pt"
    training_meta: LJSpeech_train_num_sam_625_filter_80_3.pt
    validation_meta: LJSpeech_validate_num_sam_100_filter_80_3.pt
    test_meta: LJSpeech_test_num_sam_25_filter_80_3.pt
    checksums:
      - "bde6b0af0868c7a1a9bbdaaa79d08d56"
      - "5d3b94b131c08afcca993dfbec54c63a"
      - "6702b6b7f47f12891d9d95144400bdc9"
    meta: metadata.csv
    recursive: False
    file_type: "torch"
  LJSpeechMiniRaw:
    ds_type: "audio"
    dir: "~/Dropbox/Datasets/LJSpeechSmall"
#    dir: "~/Dropbox/Datasets/LJSpeechSmall"
    format: raw
    training_meta: ljs_audio_text_train_filelist.txt
    validation_meta: ljs_audio_text_val_filelist.txt
    test_meta: ljs_audio_text_test_filelist.txt
    meta: metadata.csv
    recursive: False
    file_type: "wav"
  LJSpeechSmallPkl:
    ds_type: "audio"
    dir: "~/Dropbox/Datasets/LJSpeechSmall"
    format: tensor_mel
    training_meta: LJSpeechSmall_train_80.pt
    validation_meta: LJSpeechSmall_validate_80.pt
    test_meta: LJSpeechSmall_test_80.pt
    meta: metadata.csv
    recursive: False
    file_type: "wav"

settings:
  # debug mode
  debug:
    early_stopping: True
    epochs_log: 1000
    start_test: 10
    epochs_test: 10
    epochs_save: 10
    tensorboard_update: 10
  # baseline
  mini:
    # if we need enable early stopping
    early_stopping: True
    epochs_log: 1000
    start_test: 2
    epochs_test: 2
    epochs_save: 2
    tensorboard_update: 10
  # baseline
  baseline:
    early_stopping: True
    epochs_log: 1000
    start_test: 100
    epochs_test: 100
    epochs_save: 100
    tensorboard_update: 100
  # baseline
  medium:
    batch_size: 128
    tensorboard_update: 20      # update rate each step mod tensorboard_update
    early_stopping: True        # early stopping
    console_log_rate: 10        # when to log to console
    start_test: 20              # when to start run a validation test.
    epochs_test: 20             # epoch mod epochs_test_rate to start testing
    epochs_save: 2              # rate when to save
    save_per_iteration: False   # if we want save per iteration for large model make sense not to wait
    seed: 1234                  # fix seed
    epochs: 500                 # total epochs
    fp16: False                 # fp16 or fp32
    distributed: False          # if we distribute
    backend: "nccl"              # what distributed backend uses.
    url: "tcp://localhost:54321"
    cudnn_enabled: True
    cudnn_benchmark: False
  small:
    epochs: 500
    batch_size: 128
    grad_clipping: True
    grad_max_norm: 1.0
    tensorboard_update: 100       # update rate for tensorboard
    early_stopping: True          # early stopping
    console_log_rate: 20          # when to log to console
    start_test: 20
    predict: 50                   # will do validation , prediction every 10 iteration .(re-scale based on ds/batch size)
    predict_per_iteration: True   # use epoch or iteration counter, in large batch size we might want to see progress.
    epochs_save: 2                # when we save
    save_per_iteration: False
    seed: 1234
    fp16: False                      # fp16 run
    distributed: False               # what distributed backend uses.
    backend: "nccl"                  # backend: "gloo"
    url: "tcp://localhost:54321"
    master_address: localhost
    master_port: 54321
    cudnn_enabled: True
    cudnn_benchmark: False
    workers:
      - 192.168.254.205
    dataloader:
      train_set:
        num_workers: 1
        drop_last: True
        pin_memory: True
        shuffle: True
      validation_set:
        num_workers: 1
        drop_last: False
        pin_memory: True
        shuffle: False

optimizers:
  node_optimizer:
    eps: 1e-8
    weight_decay: 0
    amsgrad: False
    momentum=0:
    betas: [0.9, 0.999]
    type: Adam
  edge_optimizer:
    eps: 1e-8
    weight_decay: 0
    amsgrad: False
    momentum=0:
    betas: [ 0.9, 0.999 ]
    type: Adam
  tacotron2_optimizer:
    type: Adam
    weight_decay: 1e-6
    betas: [ 0.9, 0.999 ]
    learning_rate: 1e-3
    amsgrad: False
    eps: 1e-8

# lr_schedulers definition
lr_schedulers:
    - type: multistep
      milestones: [ 400, 1000 ]
      name: main_lr_scheduler
    - type: ReduceLROnPlateau
      mode: 'min'   #min, max
      patience: 10
      name: dtc_scheduler
#
# Training strategy
strategy:
  tacotron25:
    type: sequential
    order:
      - spectrogram_layer
      - wav2vec
  dts:
    type: sequential
    order:
      - spectrogram_layer
      - wav2vec

# Model definition
models:
  # this pure model specific, single model can describe both edges and nodes
  # in case we need use single model for edge and node prediction task ,
  # use keyword single_model: model_name
  tacotron25:
    spectrogram_layer:
      model: tacotron25
      optimizer: tacotron2_optimizer
#      lr_scheduler: main_lr_scheduler
      has_input: True
      has_output: True
      max_wav_value: 32768.0
      frames_per_step: 1
      sampling_rate: 22050
      filter_length: 1024   # length of the FFT window
      win_length: 1024      # each frame of audio is windowed by
      hop_length: 256
      n_mel_channels: 80
      mel_fmin: 0.0
      mel_fmax: 8000.0
      symbols_embedding_dim: 512
      encoder:
        desc: Encoder parameters
        encoder_kernel_size: 5
        encoder_n_convolutions: 3
        encoder_embedding_dim: 512
      decoder:
        n_frames_per_step: = 1
        decoder_rnn_dim: 1024
        pre_net_dim: 256
        max_decoder_steps: 1000
        gate_threshold: 0.5
        p_attention_dropout: 0.1
        p_decoder_dropout: 0.1
      attention:
        # Attention parameters
        attention_rnn_dim: 1024
        self.attention_dim: 128
      attention_location:
        # Location Layer parameters
        attention_location_n_filters: = 32
        attention_location_kernel_size: = 31
      post_net:
        desc: # Mel-post processing network parameters
        post_net_embedding_dim: 512
        post_net_kernel_size: 5
        post_net_n_convolutions: 5
    vocoder:
      state: disabled
      name: Test
      model: GraphLSTM
      optimizer: edge_optimizer
      lr_scheduler: main_lr_scheduler
      input_size: 1
  dts:
    spectrogram_layer:
      model: tacotron3
      optimizer: tacotron2_optimizer
      #      lr_scheduler: main_lr_scheduler
      has_input: True
      has_output: True
      max_wav_value: 32768.0
      frames_per_step: 1
      sampling_rate: 22050
      filter_length: 1024   # length of the FFT window
      win_length: 1024      # each frame of audio is windowed by
      hop_length: 256
      n_mel_channels: 80
      mel_fmin: 0.0
      mel_fmax: 8000.0
    vocoder:
      name: Test
      state: disabled
      model: GraphLSTM
      optimizer: edge_optimizer
      lr_scheduler: main_lr_scheduler
      input_size: 1
  GraphLstmRnn:
    node_model:
      model: GraphLSTM
      optimizer: node_optimizer
      lr_scheduler: main_lr_scheduler
      has_input: True
      has_output: True
    edge_model:
      model: GraphLSTM
      optimizer: edge_optimizer
      lr_scheduler: main_lr_scheduler
      input_size: 1


ray:
  batch_size: [32, 64, 128]
  lr_min: 1e-4
  lr_max: 1e-1
  num_samples: 10
  checkpoint_freq: 4
  resources:
    cpu: 4
    gpu: 1
  attention_location_filters: 32
  attention_kernel_size: 31
  grad_clip:
    min: 0.5
    max: 1.0
